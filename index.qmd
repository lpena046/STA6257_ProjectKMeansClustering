---
title: "K-Means Clustering"
author: "Leo Pena, Stacy, Shaif"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction


1. The Systemic Review of K-Means Clustering Algorithm

 This article explains versions of the current k-means clustering algorithm, current issues and limitations in the process, and what changes have been made to improve upon these issues. In explaining the algorithm, there is a step-by-step explanation of the traditional process. The issues and limitations mentioned are in defining the value of k, finding the initial center, reducing noise, and handling big datasets. In k-means clustering, you must define the number of clusters prior to the executing the algorithm, which is the biggest issue in dynamic datasets that change over time. The more effective you are at picking the cluster center, the less iterations you need in the process with a higher accuracy in your clusters. However, the more random the cluster center increases the randomness of the clusters, and this gets more difficult with larger and complex data structures used by major firms. Finally, noise is an issue in k-means clustering as it effects each iteration of cluster, with a heavy impact on the final cluster result. To fix the noise issue, it is assumed that determining the best possible number of clusters and finding the ideal center will reduce the noise effecting the algorithm.  There are new methods such as the hash algorithm or canopy algorithm to improve defining the center cluster. This article was a review of k-means clustering, and studies confirming the main issues surrounding the process. With our main concerns identified, we know which issues to focus on when conducting our research.
 
Ardavan Ashabi, Shamsul Bin Sahibuddin, and Mehdi Salkhordeh Haghighi. 2021. The Systematic Review of K-Means Clustering Algorithm. In Proceedings of the 2020 9th International Conference on Networks, Communication and Computing (ICNCC '20). Association for Computing Machinery, New York, NY, USA, 13–18. https://doi-org.ezproxy.lib.uwf.edu/10.1145/3447654.3447657

2. Adapting the right measures for K-Means Clustering

Previous performance validation measures are inconsistent and there is not a best available method for validating. This article studied 16 validation methods for K-means clustering, comparing the views, and explaining the defective results that may happen in the process. Some of the positive researched methods include data mining, information retrieval, machine learning, and statistics, as these can all be evaluated through the contingency matrix, which shows the frequency distribution between multiple variables, comparing the “purity” of the clusters. Some of the negative methods include the uniform effect, where the clusters are created of uniform distance from the centroid. The issues with this measure are the entropy measure (the measure of randomness), mutual information, and purity of the clusters, and are defective measures for validating k-means clusters. To improve these problems, the author normalizes the measures statistically, stating a term is valid if it has a certain value against the baseline distribution of the clusters. Finally, the author shows the math and visualization comparisons from normalized and unnormalized measures. The properties he compared were the sensitivity, the impact of the number of clusters, and summaries of the math properties of the datasets. Normalization is the necessary improvement when validating the k-means clustering algorithm. 

Junjie Wu, Hui Xiong, and Jian Chen. 2009. Adapting the right measures for K-means clustering. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '09). Association for Computing Machinery, New York, NY, USA, 877–886. https://doi-org.ezproxy.lib.uwf.edu/10.1145/1557019.1557115

3. A New Method for initializing the K-Means clustering Algorithm

K-Means clustering is very popular and used in pattern recognition and machine learning by selecting initial cluster centers and determining the distance of a cluster from the centroid for validation. One of the problems this author attempts to fix is the random selection of the initial centroid for a better set of clusters and stronger partitioning. Some of the issues with k-means clustering are that the algorithm assumes the number of clusters are known beforehand, that the algorithm is sensitive to the initial cluster center, and that it converges finitely to a local minima. A new initialization algorithm is proposed with these issues in mind. In the new algorithm, we choose small subsamples of the data and reassign the centers to the sup samples. This avoids empty clusters that lead to bad solutions. The method was used on evenly distributed data for comparison against the random selection of the initial center and the results showed improvement in effectiveness when comparing the initial points to the true solutions. It is important to note that these results were from a Gaussian dataset, which was also small compared to data collected from major firms, and the scalability of this method was unknown. 

X. Qin and S. Zheng, "A New Method for Initializing the K-Means Clustering Algorithm," 2009 Second International Symposium on Knowledge Acquisition and Modeling, Wuhan, China, 2009, pp. 41-44, doi: 10.1109/KAM.2009.20.

4. A Simple and Fast Algorithm for Global K-Means Clustering

K-means clustering is used for data mining, pattern recognition, decision support, machine learning, and image segmentation, however the normal algorithm heavy reacts to the initial chosen centroid and does not work well with large datasets. The global K-means algorithm adds one cluster center at a time through a global search procedure where N (with N being the size of the data set) is the number of executions in completed to find the ideal initial center. This article a new algorithm is presented based off the global K Means procedure and incorporates ideas from the K-medoids clustering. The new algorithm has greater performance, reduced computational times, and less influence of noise on its used test data. 
	The Global K-means algorithm does not rely on an initial center, and starting with one cluster, finds the best centroid compared to the center in the dataset. Next, the job is completed with 2 clusters and continues through the dataset until the best cluster is available as the answer to your problem. The author of the article trimmed the global algorithm down, ensured it does not depend on initial conditions and sets an optimal center at each stage, significantly reducing the computational load and making it fit for larger datasets. 
	
J. Xie and S. Jiang, "A Simple and Fast Algorithm for Global K-means Clustering," 2010 Second International Workshop on Education Technology and Computer Science, Wuhan, China, 2010, pp. 36-40, doi: 10.1109/ETCS.2010.347.


5. An Efficient K-Means Clustering Algorithm: Analysis and Implementation

The algorithm in this article is known as a filtering algorithm, in which it is based on storing data points from a KD-tree, which is a space partitioning data structure. You start with computing the kd-tree and then determining the initial centers (there is not a best practice for finding the center mentioned in this article). For every node there is a set candidate centers, and we filter out those whose center is farthest from a given Z, your chosen initial center. The analysis of time saved shows how further separation between the clusters improves the run time of the algorithm. This was practiced on synthetic data with experiments involved color quantization, vector quantization, and image segmentation. 	The filtering algorithm outperformed competing clustering algorithms, although is noted that the data set likely needs to be clustering friendly. One of the issues is that information is not passed down, so there is not a stage of the process in which the clusters closest centers are comparable (which could further improve processing time). There is work to be done in further improvement, however the filtering algorithm accomplished its task when compared to separate clustering methods. 

T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman and A. Y. Wu, "An efficient k-means clustering algorithm: analysis and implementation," in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 7, pp. 881-892, July 2002, doi: 10.1109/TPAMI.2002.1017616

6. A Fuzzy k-Modes Algorithm for Clustering Categorical Data

Fuzzy K-means algorithm means each data point is allowed to have functions in all the clusters rather than just one cluster. This article uses the fuzzy k means algorithm on categorical data by matching dissimilar features for categorical objects. This can show a transition between zones and build a practical story from the associated patterns. The authors of the article chose multiple data set to determine the performance of the algorithm. On a soybean dataset in which all columns held categorical data, three clustering algorithms were used to create four clusters and ran 100 times. The number of runs with correct classifications were higher for the fuzzy algorithm compared to a standard k means clustering algorithm, and the average computing time was much smaller for our chosen method. The next experiment measured efficiency as compared to a hard k-modes algorithm as it has been shown to work on large data sets, which is an issue with standard k means. An artificial dataset was used to test fuzzy k means by splitting into two clusters of 5000 objects. When measuring the dissimilarity using the categorical variables from the dataset it was found that hierarchal clustering was like fuzzy in terms of accuracy, however the fuzzy algorithm was much faster in completing it. So, when the dataset was large it is better to go with the fuzzy method.  

Zhexue Huang and M. K. Ng, "A fuzzy k-modes algorithm for clustering categorical data," in IEEE Transactions on Fuzzy Systems, vol. 7, no. 4, pp. 446-452, Aug. 1999, doi: 10.1109/91.784206.


## Methods

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

## Analysis and Results

### Data and Vizualisation

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

### Conlusion

## References
